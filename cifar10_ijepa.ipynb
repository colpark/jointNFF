{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-JEPA on CIFAR-10\n",
    "\n",
    "Self-contained implementation of **Image-based Joint-Embedding Predictive Architecture (I-JEPA)**  \n",
    "([Assran et al., 2023](https://arxiv.org/abs/2301.08243)) adapted for CIFAR-10 (32×32).\n",
    "\n",
    "**Architecture overview:**\n",
    "- **Encoder** — ViT-Tiny that patches the image (patch\\_size=4 → 8×8 = 64 tokens) and processes only the *context* (unmasked) patches.\n",
    "- **Target encoder** — EMA copy of the encoder; sees the full image; provides prediction targets.\n",
    "- **Predictor** — lightweight Transformer that takes context-encoder tokens + learnable mask tokens and predicts target-encoder embeddings at the masked positions.\n",
    "- **Loss** — Smooth-L1 between predicted and target embeddings (no pixel reconstruction).\n",
    "\n",
    "| Component | Details |\n",
    "|---|---|\n",
    "| Image size | 32 × 32 |\n",
    "| Patch size | 4 → 8 × 8 grid = 64 patches |\n",
    "| Encoder | dim=192, depth=6, heads=3 (~1.2 M params) |\n",
    "| Predictor | dim=96, depth=4, heads=3 (~0.3 M params) |\n",
    "| Target encoder | EMA (momentum 0.996 → 1.0) |\n",
    "| Masking | 4 target blocks (15-25 %), 1 context block (85-100 %, non-overlapping) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "from multiprocessing import Value\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Data\n",
    "DATA_ROOT      = \"./data\"\n",
    "BATCH_SIZE     = 256\n",
    "NUM_WORKERS    = 4\n",
    "IMG_SIZE       = 32\n",
    "\n",
    "# Model\n",
    "PATCH_SIZE     = 4          # 32/4 = 8 patches per side\n",
    "EMBED_DIM      = 192\n",
    "DEPTH          = 6\n",
    "NUM_HEADS      = 3\n",
    "PRED_EMBED_DIM = 96\n",
    "PRED_DEPTH     = 4\n",
    "PRED_NUM_HEADS = 3\n",
    "\n",
    "# Masking (on the 8×8 patch grid)\n",
    "ENC_MASK_SCALE  = (0.85, 1.0)   # context block covers 85-100 %\n",
    "PRED_MASK_SCALE = (0.15, 0.25)  # each target block covers 15-25 %\n",
    "ASPECT_RATIO    = (0.75, 1.5)\n",
    "N_ENC_MASKS     = 1\n",
    "N_PRED_MASKS    = 4\n",
    "MIN_KEEP        = 4\n",
    "ALLOW_OVERLAP   = False\n",
    "\n",
    "# Optimization\n",
    "EPOCHS         = 100\n",
    "LR             = 1e-3\n",
    "START_LR       = 1e-4\n",
    "FINAL_LR       = 1e-6\n",
    "WARMUP_EPOCHS  = 10\n",
    "WEIGHT_DECAY   = 0.05\n",
    "EMA_START      = 0.996\n",
    "EMA_END        = 1.0\n",
    "\n",
    "# Logging\n",
    "LOG_FREQ       = 50\n",
    "CKPT_DIR       = \"./ckpt_cifar10_ijepa\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Linear probe\n",
    "PROBE_EPOCHS   = 100\n",
    "PROBE_LR       = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Model components\n",
    "\n",
    "All modules are defined here so the notebook is fully self-contained.  \n",
    "The code mirrors Meta's I-JEPA repo (`ijepa/src/models/vision_transformer.py`) with minor simplifications for CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 — Positional embeddings & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"2-D sine-cosine positional embedding.\"\"\"\n",
    "    gh = np.arange(grid_size, dtype=float)\n",
    "    gw = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(gw, gh)  # (w, h)\n",
    "    grid = np.stack(grid, axis=0).reshape(2, 1, grid_size, grid_size)\n",
    "\n",
    "    def _1d(dim, pos):\n",
    "        assert dim % 2 == 0\n",
    "        omega = np.arange(dim // 2, dtype=float) / (dim / 2.0)\n",
    "        omega = 1.0 / 10000**omega\n",
    "        out = np.einsum(\"m,d->md\", pos.reshape(-1), omega)\n",
    "        return np.concatenate([np.sin(out), np.cos(out)], axis=1)\n",
    "\n",
    "    emb_h = _1d(embed_dim // 2, grid[0])\n",
    "    emb_w = _1d(embed_dim // 2, grid[1])\n",
    "    return np.concatenate([emb_h, emb_w], axis=1)  # (grid_size**2, embed_dim)\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, std=0.02):\n",
    "    \"\"\"Truncated normal init (PyTorch built-in is fine for modern versions).\"\"\"\n",
    "    nn.init.trunc_normal_(tensor, std=std, a=-2 * std, b=2 * std)\n",
    "\n",
    "\n",
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    Gather patches indicated by `masks` from `x`.\n",
    "    x     : (B, N, D)\n",
    "    masks : list of (B, K) index tensors\n",
    "    return: (B * len(masks), K, D)\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for m in masks:\n",
    "        idx = m.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        parts.append(torch.gather(x, 1, idx))\n",
    "    return torch.cat(parts, dim=0)\n",
    "\n",
    "\n",
    "def repeat_interleave_batch(x, B, repeat):\n",
    "    \"\"\"Repeat each length-B sub-batch `repeat` times along dim-0.\"\"\"\n",
    "    N = len(x) // B\n",
    "    return torch.cat(\n",
    "        [torch.cat([x[i * B:(i + 1) * B]] * repeat, dim=0) for i in range(N)],\n",
    "        dim=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 — Transformer building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, 3 * dim, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * mult)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(dim * mult, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, qkv_bias)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 — ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x).flatten(2).transpose(1, 2)  # (B, N, D)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"ViT encoder — processes (optionally masked) patch tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=192,\n",
    "                 depth=6, heads=3):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n",
    "        n_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, n_patches, embed_dim), requires_grad=False)\n",
    "        pe = get_2d_sincos_pos_embed(embed_dim, int(n_patches ** 0.5))\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pe).float().unsqueeze(0))\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(embed_dim, heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.apply(self._init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        \"\"\"\n",
    "        x     : (B, 3, H, W)\n",
    "        masks : None  → encode all patches\n",
    "                list of (B, K) → keep only selected patch indices\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x) + self.pos_embed  # (B, N, D)\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)              # (B·nenc, K, D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 — Predictor\n",
    "\n",
    "The predictor receives context-encoder tokens, projects them to a smaller dimension, concatenates **learnable mask tokens** (one per target position), adds positional embeddings for both sets, and runs a small Transformer. Only the mask-token outputs are returned (projected back to `embed_dim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, n_patches=64, embed_dim=192,\n",
    "                 pred_dim=96, depth=4, heads=3):\n",
    "        super().__init__()\n",
    "        self.embed_proj = nn.Linear(embed_dim, pred_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, pred_dim))\n",
    "        trunc_normal_(self.mask_token)\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, n_patches, pred_dim), requires_grad=False)\n",
    "        pe = get_2d_sincos_pos_embed(pred_dim, int(n_patches ** 0.5))\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pe).float().unsqueeze(0))\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(pred_dim, heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(pred_dim)\n",
    "        self.out_proj = nn.Linear(pred_dim, embed_dim)\n",
    "        self.apply(self._init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, ctx_tokens, masks_ctx, masks_tgt):\n",
    "        \"\"\"\n",
    "        ctx_tokens : (B·nenc, K_ctx, embed_dim)  — context encoder output\n",
    "        masks_ctx  : list of nenc tensors (B, K_ctx) — context indices\n",
    "        masks_tgt  : list of npred tensors (B, K_tgt) — target indices\n",
    "        Returns    : (B·nenc·npred, K_tgt, embed_dim)\n",
    "        \"\"\"\n",
    "        if not isinstance(masks_ctx, list): masks_ctx = [masks_ctx]\n",
    "        if not isinstance(masks_tgt, list): masks_tgt = [masks_tgt]\n",
    "\n",
    "        B = len(ctx_tokens) // len(masks_ctx)\n",
    "        npred = len(masks_tgt)\n",
    "\n",
    "        # Project to predictor dim & add context positional embeddings\n",
    "        x = self.embed_proj(ctx_tokens)                          # (B·nenc, K_ctx, pred_dim)\n",
    "        x = x + apply_masks(self.pos_embed.expand(B, -1, -1),\n",
    "                            masks_ctx)                           # positional info\n",
    "        _, N_ctx, _ = x.shape\n",
    "\n",
    "        # Build mask tokens with target positional embeddings\n",
    "        tgt_pos = apply_masks(self.pos_embed.expand(B, -1, -1),\n",
    "                              masks_tgt)                         # (B·npred, K_tgt, pred_dim)\n",
    "        tgt_pos = repeat_interleave_batch(tgt_pos, B,\n",
    "                                          repeat=len(masks_ctx)) # (B·npred·nenc, ...)\n",
    "        pred_tokens = self.mask_token.expand_as(tgt_pos) + tgt_pos\n",
    "\n",
    "        # Concat context + mask tokens\n",
    "        x = x.repeat(npred, 1, 1)                               # (B·nenc·npred, K_ctx, pred_dim)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)                  # (…, K_ctx+K_tgt, pred_dim)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = x[:, N_ctx:]            # keep only predictions\n",
    "        return self.out_proj(x)     # → (…, K_tgt, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 — Block-mask collator\n",
    "\n",
    "Generates disjoint context (encoder) and target (predictor) block masks on the 8×8 patch grid. Used as `collate_fn` in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskCollator:\n",
    "    \"\"\"\n",
    "    Dataloader collate_fn that also produces block masks.\n",
    "    Returns (images, labels), masks_enc, masks_pred.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=32, patch_size=4,\n",
    "                 enc_mask_scale=(0.85, 1.0),\n",
    "                 pred_mask_scale=(0.15, 0.25),\n",
    "                 aspect_ratio=(0.75, 1.5),\n",
    "                 nenc=1, npred=4, min_keep=4,\n",
    "                 allow_overlap=False):\n",
    "        self.h = self.w = input_size // patch_size\n",
    "        self.enc_scale = enc_mask_scale\n",
    "        self.pred_scale = pred_mask_scale\n",
    "        self.ar = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep\n",
    "        self.allow_overlap = allow_overlap\n",
    "        self._ctr = Value(\"i\", -1)\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _step(self):\n",
    "        with self._ctr.get_lock():\n",
    "            self._ctr.value += 1\n",
    "            return self._ctr.value\n",
    "\n",
    "    def _block_size(self, gen, scale, ar_scale):\n",
    "        r = torch.rand(1, generator=gen).item()\n",
    "        s = scale[0] + r * (scale[1] - scale[0])\n",
    "        n = int(self.h * self.w * s)\n",
    "        ar = ar_scale[0] + r * (ar_scale[1] - ar_scale[0])\n",
    "        bh = max(1, min(int(round(math.sqrt(n * ar))), self.h - 1))\n",
    "        bw = max(1, min(int(round(math.sqrt(n / ar))), self.w - 1))\n",
    "        return bh, bw\n",
    "\n",
    "    def _sample_mask(self, bh, bw, acceptable=None):\n",
    "        tries, timeout, max_timeout = 0, 20, 20\n",
    "        while True:\n",
    "            top = torch.randint(0, max(self.h - bh, 1), (1,))\n",
    "            left = torch.randint(0, max(self.w - bw, 1), (1,))\n",
    "            m = torch.zeros(self.h, self.w, dtype=torch.int32)\n",
    "            m[top:top + bh, left:left + bw] = 1\n",
    "            if acceptable is not None:\n",
    "                for k in range(max(len(acceptable) - tries, 0)):\n",
    "                    m *= acceptable[k]\n",
    "            idx = torch.nonzero(m.flatten()).squeeze(-1)\n",
    "            if len(idx) > self.min_keep:\n",
    "                break\n",
    "            timeout -= 1\n",
    "            if timeout == 0:\n",
    "                tries += 1\n",
    "                timeout = max_timeout\n",
    "        compl = torch.ones(self.h, self.w, dtype=torch.int32)\n",
    "        compl[top:top + bh, left:left + bw] = 0\n",
    "        return idx, compl\n",
    "\n",
    "    # --- collate ---\n",
    "    def __call__(self, batch):\n",
    "        B = len(batch)\n",
    "        collated = torch.utils.data.default_collate(batch)\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self._step())\n",
    "        p_sz = self._block_size(g, self.pred_scale, self.ar)\n",
    "        e_sz = self._block_size(g, self.enc_scale, (1.0, 1.0))\n",
    "\n",
    "        all_enc, all_pred = [], []\n",
    "        mk_pred = mk_enc = self.h * self.w\n",
    "\n",
    "        for _ in range(B):\n",
    "            masks_p, compls = [], []\n",
    "            for _ in range(self.npred):\n",
    "                idx, compl = self._sample_mask(*p_sz)\n",
    "                masks_p.append(idx)\n",
    "                compls.append(compl)\n",
    "                mk_pred = min(mk_pred, len(idx))\n",
    "            all_pred.append(masks_p)\n",
    "\n",
    "            acc = None if self.allow_overlap else compls\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                idx, _ = self._sample_mask(*e_sz, acceptable=acc)\n",
    "                masks_e.append(idx)\n",
    "                mk_enc = min(mk_enc, len(idx))\n",
    "            all_enc.append(masks_e)\n",
    "\n",
    "        # Truncate to common length & collate into tensors\n",
    "        all_pred = [[m[:mk_pred] for m in ms] for ms in all_pred]\n",
    "        all_enc  = [[m[:mk_enc]  for m in ms] for ms in all_enc]\n",
    "        masks_pred = torch.utils.data.default_collate(all_pred)  # list of npred (B, mk_pred)\n",
    "        masks_enc  = torch.utils.data.default_collate(all_enc)   # list of nenc  (B, mk_enc)\n",
    "        return collated, masks_enc, masks_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 — Learning-rate / weight-decay / momentum schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosine:\n",
    "    \"\"\"Warmup → cosine-decay LR schedule.\"\"\"\n",
    "    def __init__(self, opt, warmup, start_lr, ref_lr, final_lr, T):\n",
    "        self.opt, self.warmup = opt, warmup\n",
    "        self.start_lr, self.ref_lr, self.final_lr = start_lr, ref_lr, final_lr\n",
    "        self.T = T - warmup\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t < self.warmup:\n",
    "            lr = self.start_lr + (self.t / max(1, self.warmup)) * (self.ref_lr - self.start_lr)\n",
    "        else:\n",
    "            p = (self.t - self.warmup) / max(1, self.T)\n",
    "            lr = max(self.final_lr,\n",
    "                     self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1 + math.cos(math.pi * p)))\n",
    "        for g in self.opt.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "class CosineWD:\n",
    "    \"\"\"Cosine weight-decay schedule.\"\"\"\n",
    "    def __init__(self, opt, ref_wd, final_wd, T):\n",
    "        self.opt, self.ref_wd, self.final_wd, self.T = opt, ref_wd, final_wd, T\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        p = self.t / self.T\n",
    "        wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1 + math.cos(math.pi * p))\n",
    "        wd = max(self.final_wd, wd)\n",
    "        for g in self.opt.param_groups:\n",
    "            if not g.get(\"WD_exclude\", False):\n",
    "                g[\"weight_decay\"] = wd\n",
    "        return wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT, train=True, download=True, transform=train_transform)\n",
    "\n",
    "mask_collator = MaskCollator(\n",
    "    input_size=IMG_SIZE, patch_size=PATCH_SIZE,\n",
    "    enc_mask_scale=ENC_MASK_SCALE, pred_mask_scale=PRED_MASK_SCALE,\n",
    "    aspect_ratio=ASPECT_RATIO, nenc=N_ENC_MASKS, npred=N_PRED_MASKS,\n",
    "    min_keep=MIN_KEEP, allow_overlap=ALLOW_OVERLAP,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    drop_last=True, collate_fn=mask_collator,\n",
    ")\n",
    "\n",
    "print(f\"Training samples : {len(train_ds):,}\")\n",
    "print(f\"Batches / epoch  : {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Instantiate models & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VisionTransformer(\n",
    "    img_size=IMG_SIZE, patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM, depth=DEPTH, heads=NUM_HEADS,\n",
    ").to(DEVICE)\n",
    "\n",
    "predictor = Predictor(\n",
    "    n_patches=encoder.patch_embed.num_patches,\n",
    "    embed_dim=EMBED_DIM, pred_dim=PRED_EMBED_DIM,\n",
    "    depth=PRED_DEPTH, heads=PRED_NUM_HEADS,\n",
    ").to(DEVICE)\n",
    "\n",
    "target_encoder = copy.deepcopy(encoder).to(DEVICE)\n",
    "for p in target_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"Encoder    : {sum(p.numel() for p in encoder.parameters()):>10,} params\")\n",
    "print(f\"Predictor  : {sum(p.numel() for p in predictor.parameters()):>10,} params\")\n",
    "print(f\"Patch grid : {int(encoder.patch_embed.num_patches**0.5)}×\"\n",
    "      f\"{int(encoder.patch_embed.num_patches**0.5)} \"\n",
    "      f\"= {encoder.patch_embed.num_patches} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate param groups: no weight-decay on biases and norms\n",
    "param_groups = [\n",
    "    {\"params\": [p for n, p in encoder.named_parameters()\n",
    "                if \"bias\" not in n and len(p.shape) != 1]},\n",
    "    {\"params\": [p for n, p in predictor.named_parameters()\n",
    "                if \"bias\" not in n and len(p.shape) != 1]},\n",
    "    {\"params\": [p for n, p in encoder.named_parameters()\n",
    "                if \"bias\" in n or len(p.shape) == 1],\n",
    "     \"WD_exclude\": True, \"weight_decay\": 0},\n",
    "    {\"params\": [p for n, p in predictor.named_parameters()\n",
    "                if \"bias\" in n or len(p.shape) == 1],\n",
    "     \"WD_exclude\": True, \"weight_decay\": 0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "ipe = len(train_loader)\n",
    "T = EPOCHS * ipe\n",
    "\n",
    "lr_sched = WarmupCosine(optimizer, warmup=WARMUP_EPOCHS * ipe,\n",
    "                        start_lr=START_LR, ref_lr=LR, final_lr=FINAL_LR, T=T)\n",
    "wd_sched = CosineWD(optimizer, ref_wd=WEIGHT_DECAY,\n",
    "                     final_wd=WEIGHT_DECAY, T=T)\n",
    "\n",
    "# Momentum schedule for EMA (linear ramp)\n",
    "mom_sched = iter(\n",
    "    EMA_START + i * (EMA_END - EMA_START) / T for i in range(T + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Training loop\n",
    "\n",
    "Each step:\n",
    "1. **Target encoder** (no grad): full image → all 64 patch embeddings → select target positions → layer-norm.\n",
    "2. **Context encoder**: full image → keep only context patches (via `masks_enc`).\n",
    "3. **Predictor**: context tokens + mask tokens → predicted embeddings at target positions.\n",
    "4. **Loss**: Smooth-L1 between predicted and target embeddings.\n",
    "5. **EMA update** of target encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    predictor.train()\n",
    "    ep_loss, n = 0.0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for itr, (batch, masks_enc, masks_pred) in enumerate(train_loader):\n",
    "        imgs = batch[0].to(DEVICE, non_blocking=True)\n",
    "        m_enc  = [m.to(DEVICE) for m in masks_enc]\n",
    "        m_pred = [m.to(DEVICE) for m in masks_pred]\n",
    "\n",
    "        cur_lr = lr_sched.step()\n",
    "        cur_wd = wd_sched.step()\n",
    "\n",
    "        # ---------- forward ----------\n",
    "        # target\n",
    "        with torch.no_grad():\n",
    "            h = target_encoder(imgs)                        # (B, 64, D)\n",
    "            h = F.layer_norm(h, (h.size(-1),))\n",
    "            B = h.size(0)\n",
    "            h = apply_masks(h, m_pred)                      # (B·npred, K_tgt, D)\n",
    "            h = repeat_interleave_batch(h, B, repeat=N_ENC_MASKS)\n",
    "\n",
    "        # context → predictor\n",
    "        z = encoder(imgs, m_enc)                            # (B·nenc, K_ctx, D)\n",
    "        z = predictor(z, m_enc, m_pred)                     # (B·nenc·npred, K_tgt, D)\n",
    "\n",
    "        loss = F.smooth_l1_loss(z, h)\n",
    "\n",
    "        # ---------- backward ----------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ---------- EMA ----------\n",
    "        with torch.no_grad():\n",
    "            m = next(mom_sched)\n",
    "            for pq, pk in zip(encoder.parameters(),\n",
    "                              target_encoder.parameters()):\n",
    "                pk.data.mul_(m).add_((1 - m) * pq.detach().data)\n",
    "\n",
    "        ep_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        if itr % LOG_FREQ == 0:\n",
    "            print(f\"  [ep {epoch+1:>3d}/{EPOCHS}, itr {itr:>3d}/{ipe}] \"\n",
    "                  f\"loss={loss.item():.4f}  lr={cur_lr:.2e}\", end=\"\\r\")\n",
    "\n",
    "    avg = ep_loss / n\n",
    "    loss_history.append(avg)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Epoch {epoch+1:>3d}/{EPOCHS}  loss={avg:.4f}  \"\n",
    "          f\"lr={cur_lr:.2e}  ema={m:.5f}  ({dt:.1f}s)\")\n",
    "\n",
    "    # Save checkpoint every 20 epochs + last\n",
    "    if (epoch + 1) % 20 == 0 or (epoch + 1) == EPOCHS:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"predictor\": predictor.state_dict(),\n",
    "            \"target_encoder\": target_encoder.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": avg,\n",
    "        }, os.path.join(CKPT_DIR, f\"ckpt_ep{epoch+1}.pth\"))\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Smooth-L1 Loss\")\n",
    "plt.title(\"I-JEPA Training Loss (CIFAR-10)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 — Linear probe evaluation\n",
    "\n",
    "Freeze the (target) encoder, extract mean-pooled patch features for the whole dataset, and train a linear classifier on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data loaders (no masking, standard transforms)\n",
    "eval_norm = transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "probe_train_ds = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), eval_norm,\n",
    "    ]))\n",
    "probe_test_ds = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(), eval_norm,\n",
    "    ]))\n",
    "\n",
    "probe_train_loader = torch.utils.data.DataLoader(\n",
    "    probe_train_ds, batch_size=512, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "probe_test_loader = torch.utils.data.DataLoader(\n",
    "    probe_test_ds, batch_size=512, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(enc, loader, device):\n",
    "    \"\"\"Mean-pool all patch tokens → one feature vector per image.\"\"\"\n",
    "    feats, labels = [], []\n",
    "    enc.eval()\n",
    "    for imgs, lbl in loader:\n",
    "        h = enc(imgs.to(device))         # (B, 64, D)\n",
    "        feats.append(h.mean(dim=1).cpu())  # (B, D)\n",
    "        labels.append(lbl)\n",
    "    return torch.cat(feats), torch.cat(labels)\n",
    "\n",
    "print(\"Extracting features (target encoder)...\")\n",
    "train_feats, train_labels = extract_features(target_encoder, probe_train_loader, DEVICE)\n",
    "test_feats,  test_labels  = extract_features(target_encoder, probe_test_loader, DEVICE)\n",
    "print(f\"Train: {train_feats.shape}   Test: {test_feats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear classifier on frozen features\n",
    "linear = nn.Linear(EMBED_DIM, 10).to(DEVICE)\n",
    "probe_opt = torch.optim.SGD(linear.parameters(), lr=PROBE_LR, momentum=0.9)\n",
    "probe_sched = torch.optim.lr_scheduler.CosineAnnealingLR(probe_opt, T_max=PROBE_EPOCHS)\n",
    "\n",
    "feat_ds = torch.utils.data.TensorDataset(train_feats, train_labels)\n",
    "feat_loader = torch.utils.data.DataLoader(feat_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "probe_acc_history = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for ep in range(PROBE_EPOCHS):\n",
    "    linear.train()\n",
    "    for ft, lb in feat_loader:\n",
    "        ft, lb = ft.to(DEVICE), lb.to(DEVICE)\n",
    "        loss = F.cross_entropy(linear(ft), lb)\n",
    "        probe_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        probe_opt.step()\n",
    "    probe_sched.step()\n",
    "\n",
    "    linear.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = linear(test_feats.to(DEVICE))\n",
    "        acc = (logits.argmax(1) == test_labels.to(DEVICE)).float().mean().item() * 100\n",
    "    best_acc = max(best_acc, acc)\n",
    "    probe_acc_history.append(acc)\n",
    "\n",
    "    if (ep + 1) % 20 == 0 or ep == 0:\n",
    "        print(f\"  Probe ep {ep+1:>3d}/{PROBE_EPOCHS}  \"\n",
    "              f\"acc={acc:.2f}%  best={best_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n>>> Best linear-probe accuracy: {best_acc:.2f}%\")\n",
    "print(f\"    (random baseline = 10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(range(1, len(loss_history) + 1), loss_history, linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Smooth-L1 Loss\")\n",
    "axes[0].set_title(\"I-JEPA Pre-training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, len(probe_acc_history) + 1), probe_acc_history,\n",
    "             linewidth=1.5, color=\"tab:orange\")\n",
    "axes[1].axhline(best_acc, ls=\"--\", color=\"gray\", alpha=0.5,\n",
    "                label=f\"best = {best_acc:.1f}%\")\n",
    "axes[1].axhline(10, ls=\":\", color=\"red\", alpha=0.4, label=\"chance (10%)\")\n",
    "axes[1].set_xlabel(\"Probe Epoch\")\n",
    "axes[1].set_ylabel(\"Test Accuracy (%)\")\n",
    "axes[1].set_title(\"Linear Probe on Frozen Features\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 — Mask visualization (sanity check)\n",
    "\n",
    "Show a sample image with context (visible) and target (predicted) patch regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(img_tensor, masks_enc, masks_pred, patch_size=4, grid=8):\n",
    "    \"\"\"\n",
    "    Overlay context and target masks on a single image.\n",
    "    Green = context (encoder sees), Red = target (predictor predicts).\n",
    "    \"\"\"\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std  = np.array([0.2023, 0.1994, 0.2010])\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy() * std + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    overlay = img.copy()\n",
    "    enc_idx  = masks_enc[0][0].cpu().numpy()   # first enc mask, first sample\n",
    "    pred_idx = masks_pred[0][0].cpu().numpy()   # first pred mask, first sample\n",
    "\n",
    "    for idx in enc_idx:\n",
    "        r, c = divmod(int(idx), grid)\n",
    "        y0, x0 = r * patch_size, c * patch_size\n",
    "        overlay[y0:y0+patch_size, x0:x0+patch_size, 1] = \\\n",
    "            0.5 * overlay[y0:y0+patch_size, x0:x0+patch_size, 1] + 0.5\n",
    "\n",
    "    for idx in pred_idx:\n",
    "        r, c = divmod(int(idx), grid)\n",
    "        y0, x0 = r * patch_size, c * patch_size\n",
    "        overlay[y0:y0+patch_size, x0:x0+patch_size, 0] = \\\n",
    "            0.5 * overlay[y0:y0+patch_size, x0:x0+patch_size, 0] + 0.5\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(np.clip(overlay, 0, 1))\n",
    "    axes[1].set_title(\"Green=context, Red=target\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Grab one batch and visualize\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_imgs, sample_enc, sample_pred = sample_batch\n",
    "visualize_masks(sample_imgs[0][0], sample_enc, sample_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Metric | Expected |\n",
    "|---|---|\n",
    "| Loss (epoch 1) | ~0.15 – 0.25 |\n",
    "| Loss (epoch 100) | ~0.05 – 0.10 |\n",
    "| Linear probe (100 ep pretrain) | > 30 – 45 % |\n",
    "\n",
    "**Notes:**\n",
    "- CIFAR-10 is 32×32 — very low resolution for ViT/JEPA (designed for 224×224 ImageNet). These numbers are for sanity-checking, not SOTA.\n",
    "- Longer training (200–300 epochs) and larger models improve results.\n",
    "- The target encoder (EMA) typically gives better linear-probe features than the online encoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
